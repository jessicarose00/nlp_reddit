{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02_NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**02_NLP** \n",
    "\n",
    "- Convert text to word count vectors/frequency vectors [Countvectorize/Tfidfvectorize](https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/)\n",
    "- Remove stop words (from test?)\n",
    "- stemming, and lemmatization\n",
    "\n",
    "**03_Classification_Modeling** \n",
    "Each document is an “input” and a class label is the “output” for our predictive algorithm.\n",
    "For our $X$ variable, we will only use the `post` variable. For our $Y$ variable, we will only use the xx variable.\n",
    "\n",
    "- Train, test, split\n",
    "- Identify and explain the baseline score\n",
    "- Bayesian model\n",
    "- Logistic regression, KNN, SVM\n",
    "- Explanation of reasoning behind choosing production models\n",
    "- Evaluate model performance\n",
    "\n",
    "**Preprocessing Options**\n",
    "\n",
    "- Tokenizing\n",
    "- Regular Expression\n",
    "- Lemmatizing/Stemming\n",
    "- Cleaning (i.e. removing HTML)\n",
    "- Countvectorize\n",
    "- Tfidfvectorize\n",
    "\n",
    "**Model Options**\n",
    "\n",
    "- Logistic Regression\n",
    "- Naive Bayes (Multinomial, Bernoulli, Guassian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from bs4 import BeautifulSoup  \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-7922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>There is a search engine called [Ecosia](https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Vandana Shiva](https://youtu.be/MNM833K22LM) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>If you have a weak stomach, I wouldn’t watch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Breathing Pattern Disorders Caused by Environ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                               text\n",
       "0          0   https://www.dailymail.co.uk/news/article-7922...\n",
       "1          0  There is a search engine called [Ecosia](https...\n",
       "2          0  [Vandana Shiva](https://youtu.be/MNM833K22LM) ...\n",
       "3          0  If you have a weak stomach, I wouldn’t watch t...\n",
       "4          0   Breathing Pattern Disorders Caused by Environ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv files\n",
    "posts = pd.read_csv('posts_clean.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modeling\n",
    "X = posts['text']\n",
    "y = posts['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.525467\n",
       "0    0.474533\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check distribution of y variable\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean raw posts into a clean string of words\n",
    "\n",
    "def post_to_words(raw_text):\n",
    "    # Remove HTML with beautiful soup\n",
    "    clean_text = BeautifulSoup(raw_text).get_text()\n",
    "    \n",
    "    # Remove all non-letters with regex\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean_text)\n",
    "    \n",
    "    # Convert everything to lower case, split/tokenize into individual words.\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    # Convert the stop words to an unordered collection to improve efficiency\n",
    "    stops = set(stopwords.words('english'))\n",
    "    \n",
    "    # Remove stop words.\n",
    "    meaningful_words = [w for w in words if w not in stops]\n",
    "    \n",
    "    # Join the meaningful words back into one string separated by spaces\n",
    "    return(\" \".join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4420 posts.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of posts based on the dataframe size.\n",
    "total_posts = X_train.shape[0]\n",
    "print(f'There are {total_posts} posts.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the clean reviews.\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []\n",
    "\n",
    "print(\"Cleaning and parsing the training set movie reviews...\")\n",
    "\n",
    "# create counter j to print progress reports, helps for long code\n",
    "j = 0\n",
    "\n",
    "for train_review in X_train['review']:\n",
    "    \n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_train_reviews.append(review_to_words(train_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    # dont want to print a return message every time but it will run for a while\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "    \n",
    "    j += 1\n",
    "\n",
    "# Let's do the same for our testing set.\n",
    "\n",
    "print(\"Cleaning and parsing the testing set movie reviews...\")\n",
    "\n",
    "for test_review in X_test['review']:\n",
    "    # Convert review to words, then append to clean_train_reviews.\n",
    "    clean_test_reviews.append(review_to_words(test_review))\n",
    "    \n",
    "    # If the index is divisible by 1000, print a message\n",
    "    if (j + 1) % 1000 == 0:\n",
    "        print(f'Review {j + 1} of {total_reviews}.')\n",
    "        \n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "- Transformer: CountVectorizer\n",
    "- Estimator: Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get baseline accuracy\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "nb = MultinomialNB()\n",
    "cvec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorize on x_train data\n",
    "X_train_cvec = cvec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6644x12289 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 83565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the test\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit to train data\n",
    "nb.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = nb.predict(X_train_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_test = nb.predict(X_test_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MN Train score: 0.8749247441300422\n",
      "MN Test score: 0.6003666361136571\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy train\n",
    "print(f\"MN Cvec Train score: {metrics.accuracy_score(y_train, y_pred_train)}\")\n",
    "print(f\"MN Cvec Test score: {metrics.accuracy_score(y_test, y_pred_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
