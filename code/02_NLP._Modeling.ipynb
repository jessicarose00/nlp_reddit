{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing & Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Processing and Models\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.dailymail.co.uk/news/article-7922...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>There is a search engine called [Ecosia](https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[Vandana Shiva](https://youtu.be/MNM833K22LM) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>If you have a weak stomach, I wouldn’t watch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Breathing Pattern Disorders Caused by Environ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subreddit                                               text\n",
       "0          0   https://www.dailymail.co.uk/news/article-7922...\n",
       "1          0  There is a search engine called [Ecosia](https...\n",
       "2          0  [Vandana Shiva](https://youtu.be/MNM833K22LM) ...\n",
       "3          0  If you have a weak stomach, I wouldn’t watch t...\n",
       "4          0   Breathing Pattern Disorders Caused by Environ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv files\n",
    "posts = pd.read_csv('../data/posts_clean.csv')\n",
    "posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One more check for duplicates\n",
    "posts.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop second duplicated column\n",
    "posts.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5455, 2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape (5455, 2)\n",
    "posts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Clean the text column by removing html code \n",
    "# and making all letters lowercase\n",
    "posts[\"text\"] = posts[\"text\"].str.replace('[^a-zA-Z ]', ' ')\n",
    "posts[\"text\"] = posts[\"text\"].str.replace(r'http\\S+', '')\n",
    "posts[\"text\"] = posts[\"text\"].str.replace(r'\\[http\\S+', '')\n",
    "posts[\"text\"] = [post.lower().strip() for post in posts['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: I spent an unspeakable amount of time trying to apply these cleaning steps to the text column with .map() and .apply() but could not figure it out. Recommendations welcome!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1462650"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate dataframes for each subreddit\n",
    "env_posts = posts[posts['subreddit']==0]\n",
    "tech_posts = posts[posts['subreddit']==1]\n",
    "\n",
    "# Get character count for all posts in each subreddit\n",
    "env_char = sum([len(post) for post in env_posts['text']])\n",
    "tech_char = sum([len(post) for post in tech_posts['text']])\n",
    "\n",
    "# Check the difference in character counts\n",
    "# Env has 1,462,650 more characters\n",
    "env_char - tech_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot difference in number of posts\n",
    "objects = ('r/environment', 'r/technology')\n",
    "y_pos = np.arange(len(objects))\n",
    "height = [len(env_posts), len(tech_posts)]\n",
    "\n",
    "plt.bar(y_pos, height, align='center', alpha=0.6,)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.title('Post Origin');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot difference in number of characters\n",
    "objects = ('r/environment', 'r/technology')\n",
    "y_pos = np.arange(len(objects))\n",
    "height = [env_char, tech_char]\n",
    "\n",
    "plt.bar(y_pos, height, align='center', alpha=0.9)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Characters')\n",
    "plt.title('Subreddit Character Counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data for modeling\n",
    "X = posts['text']\n",
    "y = posts['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.52264\n",
       "0    0.47736\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check distribution of y variable\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into the training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 4091 posts in the training set.\n"
     ]
    }
   ],
   "source": [
    "# Get the number of posts for X train (4091)\n",
    "total_posts = X_train.shape[0]\n",
    "print(f'There are a total of {total_posts} posts in the training set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorize to explore common words\n",
    "cvec = CountVectorizer()\n",
    "cvec.fit(X_train)\n",
    "X_train_cvec = cvec.transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the     14174\n",
       "to      10240\n",
       "and      8688\n",
       "of       7490\n",
       "in       5192\n",
       "is       4877\n",
       "it       4094\n",
       "that     3707\n",
       "for      3614\n",
       "on       2642\n",
       "you      2575\n",
       "this     2493\n",
       "are      2403\n",
       "we       2317\n",
       "be       2297\n",
       "com      2127\n",
       "with     1968\n",
       "can      1935\n",
       "have     1899\n",
       "www      1780\n",
       "dtype: int64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put vectorized words into a df to explore - todense() will return a dense matrix\n",
    "words = pd.DataFrame(X_train_cvec.todense(), columns=cvec.get_feature_names())\n",
    "\n",
    "# check which words are appearing the most in the df\n",
    "words.sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize, Stem, Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['from',\n",
       "  'game',\n",
       "  'or',\n",
       "  'phone',\n",
       "  'interfac',\n",
       "  'vr',\n",
       "  'or',\n",
       "  'keyboard',\n",
       "  'and',\n",
       "  'mous',\n",
       "  'or',\n",
       "  'ani',\n",
       "  'other',\n",
       "  'peripher',\n",
       "  'to',\n",
       "  'ani',\n",
       "  'other',\n",
       "  'type',\n",
       "  'of',\n",
       "  'user',\n",
       "  'interfac',\n",
       "  'what',\n",
       "  'do',\n",
       "  'you',\n",
       "  'think',\n",
       "  'is',\n",
       "  'a',\n",
       "  'realli',\n",
       "  'obviou',\n",
       "  'improv',\n",
       "  'that',\n",
       "  's',\n",
       "  'miss',\n",
       "  'for',\n",
       "  'exampl',\n",
       "  'when',\n",
       "  'i',\n",
       "  'm',\n",
       "  'in',\n",
       "  'firefox',\n",
       "  'and',\n",
       "  'i',\n",
       "  'ctrl',\n",
       "  't',\n",
       "  'to',\n",
       "  'open',\n",
       "  'a',\n",
       "  'new',\n",
       "  'tab',\n",
       "  'my',\n",
       "  'cursor',\n",
       "  'is',\n",
       "  'in',\n",
       "  'the',\n",
       "  'search',\n",
       "  'bar',\n",
       "  'but',\n",
       "  'if',\n",
       "  'i',\n",
       "  'go',\n",
       "  'type',\n",
       "  'say',\n",
       "  'youtub',\n",
       "  'and',\n",
       "  'want',\n",
       "  'to',\n",
       "  'search',\n",
       "  'there',\n",
       "  'onc',\n",
       "  'it',\n",
       "  'load',\n",
       "  'now',\n",
       "  'i',\n",
       "  'have',\n",
       "  'to',\n",
       "  'take',\n",
       "  'my',\n",
       "  'hand',\n",
       "  'off',\n",
       "  'the',\n",
       "  'keyboard',\n",
       "  'to',\n",
       "  'click',\n",
       "  'the',\n",
       "  'right',\n",
       "  'field',\n",
       "  'to',\n",
       "  'start',\n",
       "  'type',\n",
       "  'or',\n",
       "  'tab',\n",
       "  'an',\n",
       "  'unknown',\n",
       "  'per',\n",
       "  'page',\n",
       "  'number',\n",
       "  'of',\n",
       "  'time',\n",
       "  'through',\n",
       "  'invis',\n",
       "  'menu',\n",
       "  'frame',\n",
       "  'that',\n",
       "  'don',\n",
       "  't',\n",
       "  'highlight',\n",
       "  'until',\n",
       "  'i',\n",
       "  'get',\n",
       "  'the',\n",
       "  'cursor',\n",
       "  'to',\n",
       "  'the',\n",
       "  'search',\n",
       "  'bar',\n",
       "  'whi',\n",
       "  'isn',\n",
       "  't',\n",
       "  'there',\n",
       "  'some',\n",
       "  'kind',\n",
       "  'of',\n",
       "  'default',\n",
       "  'focu',\n",
       "  'flag',\n",
       "  'in',\n",
       "  'css',\n",
       "  'or',\n",
       "  'on',\n",
       "  'page',\n",
       "  'in',\n",
       "  'gener',\n",
       "  'that',\n",
       "  'can',\n",
       "  'say',\n",
       "  'onc',\n",
       "  'load',\n",
       "  'in',\n",
       "  'focu',\n",
       "  'set',\n",
       "  'cursor',\n",
       "  'here',\n",
       "  'so',\n",
       "  'develop',\n",
       "  'can',\n",
       "  'tag',\n",
       "  'their',\n",
       "  'search',\n",
       "  'bar',\n",
       "  'as',\n",
       "  'their',\n",
       "  'start',\n",
       "  'point',\n",
       "  'if',\n",
       "  'there',\n",
       "  'is',\n",
       "  'whi',\n",
       "  'the',\n",
       "  'badword',\n",
       "  'don',\n",
       "  't',\n",
       "  'youtub',\n",
       "  'use',\n",
       "  'it',\n",
       "  'let',\n",
       "  's',\n",
       "  'talk',\n",
       "  'ui',\n",
       "  'what',\n",
       "  'improv',\n",
       "  'would',\n",
       "  'make',\n",
       "  'your',\n",
       "  'day',\n",
       "  'to',\n",
       "  'day',\n",
       "  'experi',\n",
       "  'better',\n",
       "  'while',\n",
       "  'use',\n",
       "  'ani',\n",
       "  'devic',\n",
       "  'or',\n",
       "  'softwar'],\n",
       " ['a',\n",
       "  'few',\n",
       "  'day',\n",
       "  'ago',\n",
       "  'i',\n",
       "  'receiv',\n",
       "  'an',\n",
       "  'interest',\n",
       "  'document',\n",
       "  'from',\n",
       "  'my',\n",
       "  'councilor',\n",
       "  'regard',\n",
       "  'the',\n",
       "  'adopt',\n",
       "  'of',\n",
       "  'the',\n",
       "  'g',\n",
       "  'mega',\n",
       "  'act',\n",
       "  'in',\n",
       "  'the',\n",
       "  'letter',\n",
       "  'i',\n",
       "  'will',\n",
       "  'describ',\n",
       "  'briefli',\n",
       "  'they',\n",
       "  'inform',\n",
       "  'that',\n",
       "  'they',\n",
       "  'will',\n",
       "  'not',\n",
       "  'amend',\n",
       "  'pem',\n",
       "  'standard',\n",
       "  'do',\n",
       "  'not',\n",
       "  'introduc',\n",
       "  'g',\n",
       "  'network',\n",
       "  'mobil',\n",
       "  'oper',\n",
       "  'do',\n",
       "  'the',\n",
       "  'bill',\n",
       "  'will',\n",
       "  'lead',\n",
       "  'to',\n",
       "  'the',\n",
       "  'liquid',\n",
       "  'of',\n",
       "  'technolog',\n",
       "  'white',\n",
       "  'spot',\n",
       "  'and',\n",
       "  'organ',\n",
       "  'disinform',\n",
       "  'campaign',\n",
       "  'also',\n",
       "  'mislead',\n",
       "  'local',\n",
       "  'govern',\n",
       "  'the',\n",
       "  'rest',\n",
       "  'of',\n",
       "  'the',\n",
       "  'document',\n",
       "  'are',\n",
       "  'also',\n",
       "  'avail',\n",
       "  'for',\n",
       "  'everyon',\n",
       "  'on',\n",
       "  'the',\n",
       "  'mc',\n",
       "  'websit',\n",
       "  'screenshot',\n",
       "  'of',\n",
       "  'first',\n",
       "  'page',\n",
       "  'document',\n",
       "  'is',\n",
       "  'in',\n",
       "  'r',\n",
       "  'polska',\n",
       "  'www',\n",
       "  'reddit',\n",
       "  'com',\n",
       "  'r',\n",
       "  'polska',\n",
       "  'comment',\n",
       "  'ccofqd',\n",
       "  'ministerstwo',\n",
       "  'cyfryzacji',\n",
       "  'walczi',\n",
       "  'z',\n",
       "  'fakenewsami',\n",
       "  'na',\n",
       "  'www',\n",
       "  'reddit',\n",
       "  'com',\n",
       "  'r',\n",
       "  'polska',\n",
       "  'comment',\n",
       "  'ccofqd',\n",
       "  'ministerstwo',\n",
       "  'cyfryzacji',\n",
       "  'walczi',\n",
       "  'z',\n",
       "  'fakenewsami',\n",
       "  'na',\n",
       "  'the',\n",
       "  'ministri',\n",
       "  'of',\n",
       "  'digit',\n",
       "  'fight',\n",
       "  'with',\n",
       "  'fake',\n",
       "  'news',\n",
       "  'about',\n",
       "  'the',\n",
       "  'g',\n",
       "  'network',\n",
       "  'and',\n",
       "  'send',\n",
       "  'educ',\n",
       "  'materi',\n",
       "  'to',\n",
       "  'local',\n",
       "  'govern']]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate tokenizer and stemmer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# Stem X_train and save words to empty list\n",
    "stemmed_words=[]\n",
    "\n",
    "for word in X_train:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    row = [p_stemmer.stem(token) for token in tokens]\n",
    "    stemmed_words.append(row)\n",
    "stemmed_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['from game or phone interfac vr or keyboard and mous or ani other peripher to ani other type of user interfac what do you think is a realli obviou improv that s miss for exampl when i m in firefox and i ctrl t to open a new tab my cursor is in the search bar but if i go type say youtub and want to search there onc it load now i have to take my hand off the keyboard to click the right field to start type or tab an unknown per page number of time through invis menu frame that don t highlight until i get the cursor to the search bar whi isn t there some kind of default focu flag in css or on page in gener that can say onc load in focu set cursor here so develop can tag their search bar as their start point if there is whi the badword don t youtub use it let s talk ui what improv would make your day to day experi better while use ani devic or softwar',\n",
       " 'a few day ago i receiv an interest document from my councilor regard the adopt of the g mega act in the letter i will describ briefli they inform that they will not amend pem standard do not introduc g network mobil oper do the bill will lead to the liquid of technolog white spot and organ disinform campaign also mislead local govern the rest of the document are also avail for everyon on the mc websit screenshot of first page document is in r polska www reddit com r polska comment ccofqd ministerstwo cyfryzacji walczi z fakenewsami na www reddit com r polska comment ccofqd ministerstwo cyfryzacji walczi z fakenewsami na the ministri of digit fight with fake news about the g network and send educ materi to local govern']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the stemmed words together and save as X_train\n",
    "X_train = [' '.join(row) for row in stemmed_words]\n",
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['thi',\n",
       "  'is',\n",
       "  'break',\n",
       "  'news',\n",
       "  'will',\n",
       "  'chang',\n",
       "  'text',\n",
       "  'onc',\n",
       "  'link',\n",
       "  'is',\n",
       "  'avail',\n",
       "  'the',\n",
       "  'ravin',\n",
       "  'bushfir',\n",
       "  'on',\n",
       "  'kangaroo',\n",
       "  'island',\n",
       "  'south',\n",
       "  'australia',\n",
       "  'ha',\n",
       "  'been',\n",
       "  'contain',\n",
       "  'after',\n",
       "  'more',\n",
       "  'than',\n",
       "  'three',\n",
       "  'week',\n",
       "  'of',\n",
       "  'activ',\n",
       "  'firefight'],\n",
       " ['my',\n",
       "  'last',\n",
       "  'post',\n",
       "  'on',\n",
       "  'thi',\n",
       "  'topic',\n",
       "  'got',\n",
       "  'delet',\n",
       "  'what',\n",
       "  'are',\n",
       "  'devic',\n",
       "  'that',\n",
       "  'onli',\n",
       "  'messag',\n",
       "  'with',\n",
       "  'no',\n",
       "  'servic',\n",
       "  'or',\n",
       "  'wifi',\n",
       "  'at',\n",
       "  'all',\n",
       "  'up',\n",
       "  'to',\n",
       "  'an',\n",
       "  'mile',\n",
       "  'rang',\n",
       "  'like',\n",
       "  'a',\n",
       "  'devic',\n",
       "  'transmitt',\n",
       "  'or',\n",
       "  'app']]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stem X_test and save words to empty list\n",
    "stemmed_words=[]\n",
    "\n",
    "for word in X_test:\n",
    "    tokens = tokenizer.tokenize(word)\n",
    "    row = [p_stemmer.stem(token) for token in tokens]\n",
    "    stemmed_words.append(row)\n",
    "stemmed_words[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi is break news will chang text onc link is avail the ravin bushfir on kangaroo island south australia ha been contain after more than three week of activ firefight',\n",
       " 'my last post on thi topic got delet what are devic that onli messag with no servic or wifi at all up to an mile rang like a devic transmitt or app']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join the stemmed words together and save as X_test\n",
    "X_test = [' '.join(row) for row in stemmed_words]\n",
    "X_test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding my own stopwords to SKlearn's set of stopwords\n",
    "stops_sk = ENGLISH_STOP_WORDS.union({'http', 'https', 'www', 'com'})\n",
    "\n",
    "# Adding my own stopwords to nltk's set of stopwords\n",
    "stops_nltk = stopwords.words('english') + ['http', 'https', 'www', 'com']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.522727\n",
       "0    0.477273\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline accuracy\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "- Transformer: CountVectorizer\n",
    "- Estimator: Logistic Regression\n",
    "- Gridsearch\n",
    "- Why: Start with a logistic regression model because it is easy to interpret and can indicate which variables are considered important by the model. Count vectorizer provides a numeric representation of how often certain words appear in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_cvec_lr = Pipeline([('cvec', CountVectorizer()),\n",
    "                         ('lr', LogisticRegression(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'cvec__max_df': 0.85, 'cvec__max_features': 3500, 'cvec__min_df': 3, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'http', 'https', 'www', 'com'], 'lr__C': 0.1}\n",
      "Best Score: 0.909803965865074\n",
      "Train Score: 0.9564898557809827\n",
      "Test Score: 0.9193548387096774\n"
     ]
    }
   ],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "# Maximum number of features fit: 2500, 3000, 3500\n",
    "# Minimum number of documents needed to include token: 2, 3\n",
    "# Maximum number of documents needed to include token: 90%, 95%\n",
    "# N-gram check individual tokens and bigrams\n",
    "# lr__C determines the strength of regularization\n",
    "\n",
    "params_cvec_lr = {\n",
    "    'cvec__stop_words':[stops_sk, stops_nltk],\n",
    "    'cvec__max_features': [2500, 3000, 3500],\n",
    "    'cvec__min_df': [2, 3],\n",
    "    'cvec__max_df': [.85, .9],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)], \n",
    "    'lr__C': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_cvec_lr = GridSearchCV(pipe_cvec_lr,\n",
    "                  param_grid=params_cvec_lr, \n",
    "                  cv=3) \n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_cvec_lr.fit(X_train, y_train)\n",
    "\n",
    "# Score model and check best params/model\n",
    "print(f'Best Params: {gs_cvec_lr.best_params_}')\n",
    "print(f'Best Score: {gs_cvec_lr.best_score_}')\n",
    "print(f'Train Score: {gs_cvec_lr.score(X_train, y_train)}')\n",
    "print(f'Test Score: {gs_cvec_lr.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "gs_cvec_lr_preds = gs_cvec_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.8679\n",
      "Sensitivity: 0.9663\n"
     ]
    }
   ],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, gs_cvec_lr_preds).ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "spec = tn / (tn + fp)\n",
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The model outperformed the baseline\n",
    "- The training set performed better than the testing set\n",
    "- hyperparameters: considering trying lower values for max_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression 2\n",
    "- Transformer: TfidfVectorizer\n",
    "- Estimator: Logistic Regression\n",
    "- Gridsearch\n",
    "- Why: TFIDFVectorizer returns a word's value proportionally to count. It is offset by the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tfidf_lr = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('lr', LogisticRegression(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'lr__C': 0.1, 'tfidf__max_df': 0.9, 'tfidf__max_features': 2500, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'http', 'https', 'www', 'com']}\n",
      "Best Score: 0.910046193600024\n",
      "Train Score: 0.929112686384747\n",
      "Test Score: 0.9252199413489736\n"
     ]
    }
   ],
   "source": [
    "# Search over the following values of hyperparameters:\n",
    "params_tfidf_lr = {\n",
    "    'tfidf__stop_words':[stops_sk, stops_nltk],\n",
    "    'tfidf__max_features': [2500, 3000, 3500],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [.9, .95],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'lr__C': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Instantiate GridSearchCV\n",
    "gs_tfidf_lr = GridSearchCV(pipe_tfidf_lr,\n",
    "                  param_grid=params_tfidf_lr, \n",
    "                  cv=3) \n",
    "\n",
    "# Fit GridSearch to training data.\n",
    "gs_tfidf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Score model and check best params/model\n",
    "print(f'Best Params: {gs_tfidf_lr.best_params_}')\n",
    "print(f'Best Score: {gs_tfidf_lr.best_score_}')\n",
    "print(f'Train Score: {gs_tfidf_lr.score(X_train, y_train)}')\n",
    "print(f'Test Score: {gs_tfidf_lr.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "gs_tfidf_lr_preds = gs_tfidf_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.8879\n",
      "Sensitivity: 0.9593\n"
     ]
    }
   ],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, gs_tfidf_lr_preds).ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "spec = tn / (tn + fp)\n",
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The model outperformed the baseline and performed better than the logistic regression model using Countvectorizer\n",
    "- The training set only performed .004% better than the test set\n",
    "- This is not great for my problem statement because it means my model is classifying very well! There is a clear distinction between the technology and environment subreddits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "- Transformer: CountVectorizer\n",
    "- Estimator: Multinomial Naive Bayes\n",
    "- Why: A Multinomial Naive Bayes model is used because count vectorizer gives us an integer count of words in a document. It's a very fast modeling algorithm and an excellent classifier, outperforming more complicated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.9643119041799071\n",
      "Test Score: 0.9332844574780058\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Naive Bayes model\n",
    "mnb = MultinomialNB()\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "# Countvectorize on x_train and x_test data\n",
    "X_train_cvec = cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)\n",
    "\n",
    "# Fit MNB to training data\n",
    "mnb.fit(X_train_cvec, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = mnb.predict(X_train_cvec)\n",
    "y_pred_test = mnb.predict(X_test_cvec)\n",
    "\n",
    "# Score model and check best params/model\n",
    "print(f'Train Score: {mnb.score(X_train_cvec, y_train)}')\n",
    "print(f'Test Score: {mnb.score(X_test_cvec, y_test)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.9739\n",
      "Sensitivity: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test).ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "spec = tn / (tn + fp)\n",
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The Multinomial Naive Bayes had a higher training score (the highest of all the models)\n",
    "- It was worth testing this model, but important to recognize it's shortcomings. Naive Bayes models assume that all of our features are independent of one another, which isn't necessarily true with language data. Certain words are more likely to follow other words, and certain words are less likely to follow other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "- Estimator: Random Forest Classifier\n",
    "- Gridsearch\n",
    "- Why: Random Forests are relatively simple to use because they require very few parameters to set and they perform pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_tfidf_rf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                         ('rf', RandomForestClassifier(random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'rf__max_depth': None, 'rf__min_samples_split': 4, 'rf__n_estimators': 50, 'tfidf__max_df': 0.9, 'tfidf__max_features': 2500, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'http', 'https', 'www', 'com']}\n",
      "Best Score: 0.9159143442567105\n",
      "Train Score: 0.9970667318504033\n",
      "Test Score: 0.9237536656891495\n"
     ]
    }
   ],
   "source": [
    "params_tfidf_rf ={\n",
    "    'tfidf__stop_words':[stops_nltk],\n",
    "    'tfidf__max_features': [2500, 3000],\n",
    "    'tfidf__min_df': [2, 3],\n",
    "    'tfidf__max_df': [.9, .95],\n",
    "    'tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'rf__n_estimators':[50,100],\n",
    "    'rf__max_depth':[None,1,2,3,4],\n",
    "    'rf__min_samples_split':[2,4,6]\n",
    "}\n",
    "\n",
    "# Instantiate Gridsearch\n",
    "gs_tfidf_rf = GridSearchCV(pipe_tfidf_rf, \n",
    "                     param_grid=params_tfidf_rf, \n",
    "                     cv=3)\n",
    "\n",
    "# Fit Gridsearch to the training data\n",
    "gs_tfidf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Score model and check best params/model\n",
    "print(f'Best Params: {gs_tfidf_rf.best_params_}')\n",
    "print(f'Best Score: {gs_tfidf_rf.best_score_}')\n",
    "print(f'Train Score: {gs_tfidf_rf.score(X_train, y_train)}')\n",
    "print(f'Test Score: {gs_tfidf_rf.score(X_test, y_test)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "gs_tfidf_rf_preds = gs_tfidf_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specificity: 0.9278\n",
      "Sensitivity: 0.9201\n"
     ]
    }
   ],
   "source": [
    "# Save TN/FP/FN/TP values.\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, gs_tfidf_rf_preds).ravel()\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "spec = tn / (tn + fp)\n",
    "sens = tp / (tp + fn)\n",
    "\n",
    "print(f'Specificity: {round(spec,4)}')\n",
    "print(f'Sensitivity: {round(sens,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- The Random Forest Model is overfit, indicated by the significantly higher training score. This can also be concluded because the best max_depth parameter is 'None', which indicates an overfit tree.\n",
    "- As we increased variation by adding the min samples split parameter, we saw a slight decrease in the model's performance. The min_samples_split parameter could be driving max_depth to none. \n",
    "- Parameters indicate a lower number of trees is ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Preprocessing and EDA  \n",
    "https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python  \n",
    "https://pythonspot.com/matplotlib-bar-chart/  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.989px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
